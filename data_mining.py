# -*- coding: utf-8 -*-
"""data_mining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iPGXWKTrcn9-K1k43aAOkwrWr8baZiMA
"""

#Import packages

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import sklearn.decomposition as PCA
import sklearn.preprocessing as StandardScaler
import scipy.stats as stats

"""Variables (Colonnes) :

50 Colonnes de Gènes : Chaque colonne représente le niveau d'expression d'un gène spécifique (par exemple, NAT1, BIRC5, BAG1, etc.). Ces niveaux d'expression sont mesurés pour chaque échantillon.

1 Colonne de Sous-Type Moléculaire (pam50) : Cette colonne contient le sous-type moléculaire du cancer du sein pour chaque échantillon.

Observations (Lignes) :

1016 Échantillons : Chaque ligne représente un échantillon de cancer du sein, avec les niveaux d'expression des 50 gènes et le sous-type moléculaire correspondant.

Le sous-type moléculaire est une classification du cancer du sein basée sur l'expression des gènes. Quatre sous-types sont présents dans ce dataset :

Luminal-A : Généralement associé à un bon pronostic. Ce sous-type est souvent sensible aux thérapies hormonales.

Luminal-B : Aussi sensible aux thérapies hormonales, mais avec un pronostic légèrement moins favorable que Luminal-A.

HER2-Enriched : Caractérisé par une surexpression du gène HER2. Ce sous-type peut être traité avec des thérapies ciblant HER2.

Basal-Like : Souvent associé à un pronostic sombre. Ce sous-type est généralement triple négatif (absence de récepteurs hormonaux et de surexpression de HER2).
"""

import pandas as pd

df = pd.read_csv('breast_cancer_dataset.csv', sep=';', index_col='id_sample')

df.head()

print("\nRésumé général du dataset :")
print(df.info())  # Donne des informations sur les colonnes, types de données, valeurs non nulles

df.shape

print("\nStatistiques descriptives :")
df.describe(include='all')  # Statistiques pour les colonnes numériques et catégoriques

# Description des variables qualitatives
df.describe(include="object")

print("\nVérification des valeurs nulles par colonne :")
print(df.isnull().sum())  # Compte les valeurs nulles dans chaque colonne

print("\nColonnes complètement vides :")
colonnes_vides = df.columns[df.isnull().all()]
print(colonnes_vides)

df.duplicated().any()
print("\nNombre de lignes dupliquées :")
print(df.duplicated().sum())  # Compte les lignes dupliquées

# Exclusion de la colonne 'pam50' pour la matrice de corrélation
df_cleaned = df.drop(columns=["pam50"])

# Calcul de la matrice de corrélation
correlation_matrix = df_cleaned.corr()



# Visualisation avec une heatmap Seaborn
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Matrice de corrélation (sans colonne pam50)')
plt.show()

print("\nTypes de données uniques dans chaque colonne :")
for column in df.columns:
    print(f"{column}: {df[column].nunique()} valeurs uniques")

# Distribution des niveaux d'expression des gènes
plt.figure(figsize=(12, 6))
sns.boxplot(data=df_cleaned, orient='h')
plt.title('Distribution des Niveaux d\'Expression des Gènes')
plt.show()

df.groupby(['pam50']).size()

df.groupby(['pam50']).size().plot(kind = "bar")

# Données d'expression de 50 gènes
X = df.select_dtypes('number')
print('X', X.shape)

# Etiquettes correspondantes (sous-types moléculaires)
y = df['pam50']
print('y', y.shape)

"""# Analyse en Composantes principales"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler() # instanciation de l'objet scaler
X_scaled = scaler.fit_transform(X) # normalisation centrée-réduite
X_scaled = pd.DataFrame(X_scaled, index=X.index, columns=X.columns) # conversion du résultat en objet dataframe de pandas

from sklearn.decomposition import PCA

pca = PCA() # instanciation de l'objet pca
X_pca = pca.fit_transform(X_scaled) # réalisation de l'ACP sur les données X_scaled

# Conversion en dataframe pandas
pca_columns = ['PC' + str(c) for c in range(1, X_pca.shape[1]+1, 1)] # création d'une liste avec les noms de colonnes de PC1 à PC50
X_pca = pd.DataFrame(X_pca, index=X.index, columns=pca_columns) # création du dataframe
X_pca.head()

pca.explained_variance_ratio_

explained_variance = pd.Series(dict(zip(X_pca.columns, 100.0*pca.explained_variance_ratio_)))
print(explained_variance.head())

explained_variance.plot(kind='bar', figsize=(15, 4), rot=90, ylabel='Explained variance')

coeff = np.transpose(pca.components_[0:2, :])  # Transposition pour avoir les variables en lignes
n = coeff.shape[0]  # Nombre de variables

# Créer la figure
plt.figure(figsize=(10, 10))

# Tracer les vecteurs des variables
for i in range(n):
    plt.arrow(0, 0, coeff[i, 0], coeff[i, 1], color='k', alpha=0.9, head_width=0.02)
    plt.text(coeff[i, 0] * 1.15, coeff[i, 1] * 1.15, df_cleaned.columns[i], color='k', ha='center', va='center')

# Tracer le cercle unitaire
circle = plt.Circle((0, 0), 1, color='gray', fill=False, linestyle='--')
plt.gca().add_artist(circle)

# Ajuster les limites et les axes
plt.xlim(-1, 1)
plt.ylim(-1, 1)
plt.axhline(0, color='gray', linewidth=1)
plt.axvline(0, color='gray', linewidth=1)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Cercle de corrélation ACP')

# Afficher la figure
plt.show()

X_pca.plot(x='PC1', y='PC2', kind='scatter', figsize=(5, 5), color='gray')

dict_colors = {'luminal-A': 'forestgreen', 'luminal-B': 'royalblue', 'HER2-enriched': 'orange', 'basal-like': 'crimson'}
y_colors = [dict_colors[yi] for yi in y]
X_pca.plot(x='PC1', y='PC2', kind='scatter', figsize=(5, 5), color=y_colors)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(5, 5))
ax = fig.add_subplot(projection='3d')
ax.scatter(X_pca['PC1'], X_pca['PC2'], X_pca['PC3'], marker='o', s=30, edgecolor='k', facecolor=y_colors)
ax.set_xlabel('PC1 - ' + '{:.1f}%'.format(explained_variance['PC1']))
ax.set_ylabel('PC2 - ' + '{:.1f}%'.format(explained_variance['PC2']))
ax.set_zlabel('PC3 - ' + '{:.1f}%'.format(explained_variance['PC3']))
ax.view_init(elev=15, azim=45)

"""# Visualiser les données avec la méthode t-SNE

"""

from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, init='pca', random_state=0, n_jobs=-1)
X_tsne = tsne.fit_transform(X_scaled)

columns = ['DIM' + str(c) for c in range(1, X_tsne.shape[1]+1, 1)]
X_tsne = pd.DataFrame(X_tsne, index=X.index, columns=columns)
X_tsne.head()

X_tsne.plot(x='DIM1', y='DIM2', kind='scatter', figsize=(5, 5), color=y_colors)

# La divergence de Kullback-Leibler après optimisation
tsne.kl_divergence_

"""# Visualiser les données avec la méthode UMAP"""

!pip install umap-learn

import umap
embedding = umap.UMAP(n_components=2, random_state=0, n_jobs=-1)
X_umap = embedding.fit_transform(X_scaled)
columns = ['DIM' + str(c) for c in range(1, X_umap.shape[1]+1, 1)]
X_umap = pd.DataFrame(X_umap, index=X.index, columns=columns)

X_umap.plot(x='DIM1', y='DIM2', kind='scatter', figsize=(5, 5), color=y_colors)

#3d
embedding = umap.UMAP(n_components=3, random_state=0, n_jobs=-1)
X_umap = embedding.fit_transform(X_scaled)
columns = ['DIM' + str(c) for c in range(1, X_umap.shape[1]+1, 1)]
X_umap = pd.DataFrame(X_umap, index=X.index, columns=columns)

fig = plt.figure(figsize=(5, 5))
ax = fig.add_subplot(projection='3d')
ax.scatter(X_umap['DIM1'], X_umap['DIM2'], X_umap['DIM3'], marker='o', s=30, edgecolor='k', facecolor=y_colors)
ax.set_xlabel('DIM1')
ax.set_ylabel('DIM2')
ax.set_zlabel('DIM3')
ax.view_init(elev=15, azim=45)

"""# Classification Ascendante Hiérarchique (CAH)"""

# librairies pour la CAH
from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
cancer_cr = ss.fit_transform(df_cleaned)

# générer la matrice des liens
Z = linkage(cancer_cr,method='ward', metric='euclidean')

Z.shape
#affichage du dendogramme
plt.title("CAH")
dendrogram(Z, color_threshold=0)
plt.show()

Z.shape

df.index

# matérialisation des 4 classes (hauteur t=55)
plt.title('CAH avec matérialisation des 4 classes')
dendrogram(Z, orientation='top', color_threshold=55)
plt.show()

#Découpage à la hauteur t=7 ==> identification de 4 groupes obtenus
from scipy.cluster.hierarchy import fcluster
groupes_cah = fcluster(Z, t=55, criterion='distance')
#print(groupes_cah)

#index triés des groupes
import numpy as np
idg = np.argsort(groupes_cah)

#Affichage des observations et leurs groupes
import pandas
print(pandas.DataFrame(df_cleaned.index[idg],groupes_cah[idg]))

"""# Méthodes des centres mobiles : K-means"""

#Méthode de l'Inertie (Elbow Method)
from sklearn import cluster
max_k = 12

inertia= []
k_values = range(2, max_k +1)

for k in k_values:
    kmeans = cluster.KMeans(n_clusters=k,random_state=42, max_iter=100)
    kmeans.fit(cancer_cr)
    inertia.append(kmeans.inertia_)

plt.plot(k_values, inertia, 'bo-')

#Méthode de la Silhouette
#librairie pour évaluation des partitions
from sklearn import metrics
#utilisation de la métrique "silhouette"
#faire varier le nombre de clusters de 2 à 10
res= np.arange(9, dtype ="double")
for k in np.arange(9):
    km = cluster.KMeans(n_clusters=k+2)
    km.fit(cancer_cr)
    res[k] = metrics.silhouette_score(cancer_cr,km.labels_)
print(res)

#graphique
import matplotlib.pyplot as plt
plt.title("Sihouette")
plt.xlabel("# of clusters")
plt.plot(np.arange(2,11,1),res)
plt.show()

#K-means sur les données centrées et réduites
kmeans = cluster.KMeans(n_clusters=4)
kmeans.fit(cancer_cr)

#index triés des groupes
idk = np.argsort(kmeans.labels_)

#affichage des observations et leurs groupes
print(pandas.DataFrame(df.index[idk],kmeans.labels_[idk]))

#K-means sur les données centrées et réduites
from sklearn import cluster
kmeans = cluster.KMeans(n_clusters=4, random_state=42, max_iter=1000)
cluster_labels= kmeans.fit_predict(cancer_cr)

cluster_labels

df_cleaned["cluster"]=cluster_labels

df_cleaned

cluster_colors = {0: 'forestgreen', 1: 'royalblue', 2: 'orange', 3: 'crimson'}
y_colors = [cluster_colors[yi] for yi in cluster_labels]

fig = plt.figure(figsize=(5, 5))
ax = fig.add_subplot(projection='3d')
ax.scatter(X_pca['PC1'], X_pca['PC2'], X_pca['PC3'], marker='o', s=30, edgecolor='k', facecolor=y_colors)

"""# DBSCAN"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors

# Function to plot k-distance graph
def plot_k_distance_graph(X, k):
    neigh = NearestNeighbors(n_neighbors=k)
    neigh.fit(X)
    distances, _ = neigh.kneighbors(X)
    distances = np.sort(distances[:, k-1])
    plt.figure(figsize=(10, 6))
    plt.plot(distances)
    plt.xlabel('Points')
    plt.ylabel(f'{k}-th nearest neighbor distance')
    plt.title('K-distance Graph')
    plt.show()
# Plot k-distance graph
plot_k_distance_graph(X, k=4)

# Perform DBSCAN clustering
epsilon = 5 # Chosen based on k-distance graph
min_samples = 5  # 2 * num_features (2D data)
dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)
clusters = dbscan.fit_predict(X)

dbscan.labels_

# Print number of clusters and noise points
n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
n_noise = list(clusters).count(-1)
print(f'Number of clusters: {n_clusters}')
print(f'Number of noise points: {n_noise}')

"""# model prediction"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


# Séparer les variables indépendantes et la variable cible
X = df.drop('pam50', axis=1)  # Remplacez 'pam50' par le nom de votre colonne cible
y = df['pam50']

#imbalanced data
y.value_counts()

y.value_counts().plot.pie(autopct='%2f')

#Random Undersampling
from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(sampling_strategy="not minority") # String
X_res, y_res = rus.fit_resample(X, y)

ax = y_res.value_counts().plot.pie(autopct='%.2f')
_ = ax.set_title("Under-sampling")

# Class distribution
y_res.value_counts()

from imblearn.over_sampling import RandomOverSampler

#ros = RandomOverSampler(sampling_strategy=1) # Float
ros = RandomOverSampler(sampling_strategy="not majority") # String
X_res, y_res = ros.fit_resample(X, y)

ax = y_res.value_counts().plot.pie(autopct='%.2f')
_ = ax.set_title("Over-sampling")

y_res.value_counts()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

# Définir les classifieurs
classifier = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree Classifier": DecisionTreeClassifier(),
    "Random Forest Classifier": RandomForestClassifier()
}

# Boucle pour entraîner et évaluer chaque classifieur
for name, clf in classifier.items():
    print(f"\n========== {name} ===========")

    # Entraîner le modèle
    clf.fit(X_train, y_train)

    # Prédire sur l'ensemble de test
    y_pred = clf.predict(X_test)

    # Calculer les métriques
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')  # Utiliser 'weighted' pour multiclasse
    recall = recall_score(y_test, y_pred, average='weighted')        # Utiliser 'weighted' pour multiclasse
    f1 = f1_score(y_test, y_pred, average='weighted')                # Utiliser 'weighted' pour multiclasse

    # Afficher les résultats
    print(f"\n Accuracy: {accuracy}")
    print(f"\n Precision: {precision}")
    print(f"\n Recall: {recall}")
    print(f"\n F1 Score: {f1}")

